---
title: "Privacy Preservation with Noise in Explainable AI"
collection: publications
category: conferences
permalink: /publication/2025-08-27-conference-paper-1
excerpt: 'This paper evaluates the use of noise of different distributions in protecting sensitive information in Explainable AI systems.'
date: 2025-08-27
venue: '22nd Annual International Conference on Privacy, Security and Trust (PST)'
citation: 'Allana, S. & Dara, R. Privacy Preservation with Noise in Explainable AI in 22nd Annual International Conference on Privacy, Security and Trust (PST) (2025), 1â€“8.'
---

Black-box Artificial Intelligence (AI) systems have achieved state-of-the-art accuracy in many problem domains in recent years. However, the lack of transparency ofthese systems is a bottleneck in their usage in high-risk applications which directly impact individuals. Trustworthy AI proposes principles such as reliability, validity, privacy, fairness, and explainability among others to mitigate risks from large-scale AI deployments. Explainable AI (XAI) is a technique of providing insights into the decision-making process of black-box systems thus enabling transparency. It plays a crucial role in communicating the rationale of automated decisions to relevant stakeholders. Though it is a highly desirable requirement, recent research has determined that explanations can introduce new privacy risks in AI systems. Different types of privacy attacks are shown on XAI deployed in production and cloud systems. Despite these risks, currently there is lack of research in defenses for known privacy attacks in XAI. In this article, we contribute to this gap by researching a defense mechanism for attribute inference on feature-based XAI. We empirically evaluate a well-known privacy preservation technique, namely, additive noise, and show its impact on privacy, explainability and utility. Our findings indicate that additive noise enables privacy while achieving faithful explanations without compromising model utility.